{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a8051a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "try:\n",
    "    import networkx as nx\n",
    "    from networkx.algorithms.bipartite.matrix import from_biadjacency_matrix\n",
    "except ImportError:\n",
    "    nx = None\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer, XLMModel, XLMTokenizer, RobertaModel, RobertaTokenizer, XLMRobertaModel, XLMRobertaTokenizer, AutoConfig, AutoModel, AutoTokenizer\n",
    "\n",
    "    \n",
    "\n",
    "class EmbeddingLoader(object):\n",
    "    def __init__(self, model: str=\"bert-base-multilingual-cased\", model_path: str = \"bert-base-multilingual-cased\", device=torch.device('cpu'), layer: int=8):\n",
    "        TR_Models = {\n",
    "            'bert-base-uncased': (BertModel, BertTokenizer),\n",
    "            'bert-base-multilingual-cased': (BertModel, BertTokenizer),\n",
    "            'bert-base-multilingual-uncased': (BertModel, BertTokenizer),\n",
    "            'xlm-mlm-100-1280': (XLMModel, XLMTokenizer),\n",
    "            'roberta-base': (RobertaModel, RobertaTokenizer),\n",
    "            'xlm-roberta-base': (XLMRobertaModel, XLMRobertaTokenizer),\n",
    "            'xlm-roberta-large': (XLMRobertaModel, XLMRobertaTokenizer),\n",
    "        }\n",
    "\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.layer = layer\n",
    "        self.emb_model = None\n",
    "        self.tokenizer = None\n",
    "\n",
    "        model_class, tokenizer_class = TR_Models[model]\n",
    "        self.emb_model = model_class.from_pretrained(model_path, output_hidden_states=True)\n",
    "        self.emb_model.eval()\n",
    "        self.emb_model.to(self.device)\n",
    "        self.tokenizer = tokenizer_class.from_pretrained(model_path)\n",
    "        print(\"Initialized the EmbeddingLoader with model: {}\".format(self.model))\n",
    "    \n",
    "    def get_embed_list(self, sent_batch: List[List[str]]) -> torch.Tensor:\n",
    "        if self.emb_model is not None:\n",
    "            with torch.no_grad():\n",
    "                if not isinstance(sent_batch[0], str):\n",
    "                    inputs = self.tokenizer(sent_batch, is_split_into_words=True, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "                else:\n",
    "                    inputs = self.tokenizer(sent_batch, is_split_into_words=False, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "                outputs = self.emb_model(**inputs.to(self.device))[2][self.layer]\n",
    "\n",
    "                return outputs[:, 1:-1, :]\n",
    "        else:\n",
    "            return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90bec2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SentenceAligner(object):\n",
    "    def __init__(self, model: str = \"bert\", model_path: str = \"bert\", token_type: str = \"bpe\", distortion: float = 0.0, matching_methods: str = \"mai\", device: str = \"cpu\", layer: int = 8):\n",
    "        model_names = {\n",
    "            \"bert\": \"bert-base-multilingual-cased\",\n",
    "            \"xlmr\": \"xlm-roberta-base\"\n",
    "            }\n",
    "        all_matching_methods = {\"a\": \"inter\", \"m\": \"mwmf\", \"i\": \"itermax\", \"f\": \"fwd\", \"r\": \"rev\"}\n",
    "\n",
    "        self.model = model\n",
    "        if model in model_names:\n",
    "            self.model = model_names[model]\n",
    "        self.token_type = token_type\n",
    "        self.distortion = distortion\n",
    "        self.matching_methods = [all_matching_methods[m] for m in matching_methods]\n",
    "        self.device = torch.device(device)\n",
    "\n",
    "        self.embed_loader = EmbeddingLoader(model=self.model, model_path=model_path, device=self.device, layer=layer)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_max_weight_match(sim: np.ndarray) -> np.ndarray:\n",
    "        if nx is None:\n",
    "            raise ValueError(\"networkx must be installed to use match algorithm.\")\n",
    "        def permute(edge):\n",
    "            if edge[0] < sim.shape[0]:\n",
    "                return edge[0], edge[1] - sim.shape[0]\n",
    "            else:\n",
    "                return edge[1], edge[0] - sim.shape[0]\n",
    "        G = from_biadjacency_matrix(csr_matrix(sim))\n",
    "        matching = nx.max_weight_matching(G, maxcardinality=True)\n",
    "        matching = [permute(x) for x in matching]\n",
    "        matching = sorted(matching, key=lambda x: x[0])\n",
    "        res_matrix = np.zeros_like(sim)\n",
    "        for edge in matching:\n",
    "            res_matrix[edge[0], edge[1]] = 1\n",
    "        return res_matrix\n",
    "\n",
    "    @staticmethod\n",
    "    def get_similarity(X: np.ndarray, Y: np.ndarray) -> np.ndarray:\n",
    "        return (cosine_similarity(X, Y) + 1.0) / 2.0\n",
    "\n",
    "    @staticmethod\n",
    "    def average_embeds_over_words(bpe_vectors: np.ndarray, word_tokens_pair: List[List[str]]) -> List[np.array]:\n",
    "        w2b_map = []\n",
    "        cnt = 0\n",
    "        w2b_map.append([])\n",
    "        for wlist in word_tokens_pair[0]:\n",
    "            w2b_map[0].append([])\n",
    "            for x in wlist:\n",
    "                w2b_map[0][-1].append(cnt)\n",
    "                cnt += 1\n",
    "        cnt = 0\n",
    "        w2b_map.append([])\n",
    "        for wlist in word_tokens_pair[1]:\n",
    "            w2b_map[1].append([])\n",
    "            for x in wlist:\n",
    "                w2b_map[1][-1].append(cnt)\n",
    "                cnt += 1\n",
    "\n",
    "        new_vectors = []\n",
    "        for l_id in range(2):\n",
    "            w_vector = []\n",
    "            for word_set in w2b_map[l_id]:\n",
    "                w_vector.append(bpe_vectors[l_id][word_set].mean(0))\n",
    "            new_vectors.append(np.array(w_vector))\n",
    "        return new_vectors\n",
    "\n",
    "    @staticmethod\n",
    "    def get_alignment_matrix(sim_matrix: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        m, n = sim_matrix.shape\n",
    "        forward = np.eye(n)[sim_matrix.argmax(axis=1)]  # m x n\n",
    "        backward = np.eye(m)[sim_matrix.argmax(axis=0)]  # n x m\n",
    "        return forward, backward.transpose()\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_distortion(sim_matrix: np.ndarray, ratio: float = 0.5) -> np.ndarray:\n",
    "        shape = sim_matrix.shape\n",
    "        if (shape[0] < 2 or shape[1] < 2) or ratio == 0.0:\n",
    "            return sim_matrix\n",
    "\n",
    "        pos_x = np.array([[y / float(shape[1] - 1) for y in range(shape[1])] for x in range(shape[0])])\n",
    "        pos_y = np.array([[x / float(shape[0] - 1) for x in range(shape[0])] for y in range(shape[1])])\n",
    "        distortion_mask = 1.0 - ((pos_x - np.transpose(pos_y)) ** 2) * ratio\n",
    "\n",
    "        return np.multiply(sim_matrix, distortion_mask)\n",
    "\n",
    "    @staticmethod\n",
    "    def iter_max(sim_matrix: np.ndarray, max_count: int=2) -> np.ndarray:\n",
    "        alpha_ratio = 0.9\n",
    "        m, n = sim_matrix.shape\n",
    "        forward = np.eye(n)[sim_matrix.argmax(axis=1)]  # m x n\n",
    "        backward = np.eye(m)[sim_matrix.argmax(axis=0)]  # n x m\n",
    "        inter = forward * backward.transpose()\n",
    "\n",
    "        if min(m, n) <= 2:\n",
    "            return inter\n",
    "\n",
    "        new_inter = np.zeros((m, n))\n",
    "        count = 1\n",
    "        while count < max_count:\n",
    "            mask_x = 1.0 - np.tile(inter.sum(1)[:, np.newaxis], (1, n)).clip(0.0, 1.0)\n",
    "            mask_y = 1.0 - np.tile(inter.sum(0)[np.newaxis, :], (m, 1)).clip(0.0, 1.0)\n",
    "            mask = ((alpha_ratio * mask_x) + (alpha_ratio * mask_y)).clip(0.0, 1.0)\n",
    "            mask_zeros = 1.0 - ((1.0 - mask_x) * (1.0 - mask_y))\n",
    "            if mask_x.sum() < 1.0 or mask_y.sum() < 1.0:\n",
    "                mask *= 0.0\n",
    "                mask_zeros *= 0.0\n",
    "\n",
    "            new_sim = sim_matrix * mask\n",
    "            fwd = np.eye(n)[new_sim.argmax(axis=1)] * mask_zeros\n",
    "            bac = np.eye(m)[new_sim.argmax(axis=0)].transpose() * mask_zeros\n",
    "            new_inter = fwd * bac\n",
    "\n",
    "            if np.array_equal(inter + new_inter, inter):\n",
    "                break\n",
    "            inter = inter + new_inter\n",
    "            count += 1\n",
    "        return inter\n",
    "\n",
    "    def get_word_aligns(self, src_sent: Union[str, List[str]], trg_sent: Union[str, List[str]]) -> Dict[str, List]:\n",
    "        if isinstance(src_sent, str):\n",
    "            src_sent = src_sent.split()\n",
    "        if isinstance(trg_sent, str):\n",
    "            trg_sent = trg_sent.split()\n",
    "        l1_tokens = [self.embed_loader.tokenizer.tokenize(word) for word in src_sent]\n",
    "        l2_tokens = [self.embed_loader.tokenizer.tokenize(word) for word in trg_sent]\n",
    "        bpe_lists = [[bpe for w in sent for bpe in w] for sent in [l1_tokens, l2_tokens]]\n",
    "\n",
    "        if self.token_type == \"bpe\":\n",
    "            l1_b2w_map = []\n",
    "            for i, wlist in enumerate(l1_tokens):\n",
    "                l1_b2w_map += [i for x in wlist]\n",
    "            l2_b2w_map = []\n",
    "            for i, wlist in enumerate(l2_tokens):\n",
    "                l2_b2w_map += [i for x in wlist]\n",
    "\n",
    "        vectors = self.embed_loader.get_embed_list([src_sent, trg_sent]).cpu().detach().numpy()\n",
    "        vectors = [vectors[i, :len(bpe_lists[i])] for i in [0, 1]]\n",
    "\n",
    "        if self.token_type == \"word\":\n",
    "            vectors = self.average_embeds_over_words(vectors, [l1_tokens, l2_tokens])\n",
    "\n",
    "        all_mats = {}\n",
    "        sim = self.get_similarity(vectors[0], vectors[1])\n",
    "        sim = self.apply_distortion(sim, self.distortion)\n",
    "\n",
    "        all_mats[\"fwd\"], all_mats[\"rev\"] = self.get_alignment_matrix(sim)\n",
    "        all_mats[\"inter\"] = all_mats[\"fwd\"] * all_mats[\"rev\"]\n",
    "        if \"mwmf\" in self.matching_methods:\n",
    "            all_mats[\"mwmf\"] = self.get_max_weight_match(sim)\n",
    "        if \"itermax\" in self.matching_methods:\n",
    "            all_mats[\"itermax\"] = self.iter_max(sim)\n",
    "\n",
    "        aligns = {x: set() for x in self.matching_methods}\n",
    "        for i in range(len(vectors[0])):\n",
    "            for j in range(len(vectors[1])):\n",
    "                for ext in self.matching_methods:\n",
    "                    if all_mats[ext][i, j] > 0:\n",
    "                        if self.token_type == \"bpe\":\n",
    "                            #aligns[ext].add((l1_b2w_map[i], l2_b2w_map[j]))\n",
    "                            aligns[ext].add(str(l1_b2w_map[i]) + \"-\" + str(l2_b2w_map[j]))\n",
    "                        else:\n",
    "                            aligns[ext].add(str(i) + \"-\" + str(j))\n",
    "        for ext in aligns:\n",
    "            aligns[ext] = sorted(aligns[ext])\n",
    "            aligns[ext] = \" \".join(aligns[ext])\n",
    "        return aligns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "988ef23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/lisan/code/bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized the EmbeddingLoader with model: bert-base-multilingual-cased\n",
      "{'mwmf': '0-3 1-0 10-11 2-1 3-2 3-3 4-4 5-10 5-8 5-9 6-5 8-6 9-7', 'inter': '1-0 10-11 2-1 3-3 4-4 5-10 5-9 6-5 8-6 9-7', 'itermax': '0-0 1-0 10-11 2-1 3-3 4-4 5-10 5-8 5-9 6-5 8-6 9-7'}\n"
     ]
    }
   ],
   "source": [
    "source_sentence = \"Sir Nils Olav III. was knighted by the norwegian king .\"\n",
    "target_sentence = \"Nils Olav der Dritte wurde vom norwegischen König zum Ritter geschlagen .\"\n",
    "\n",
    "device =\"cpu\"\n",
    "model = \"bert-base-multilingual-cased\"\n",
    "model_path = \"/Users/lisan/code/bert-base-multilingual-cased\"\n",
    "\n",
    "\n",
    "\n",
    "model = SentenceAligner(model=model, model_path=model_path, device=device)\n",
    "result = model.get_word_aligns(source_sentence, target_sentence)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "48b3ac5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/lisan/code/bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized the EmbeddingLoader with model: bert-base-multilingual-cased\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n"
     ]
    }
   ],
   "source": [
    "\n",
    "root = \"/Users/lisan/code/AlignMan/examples/\"\n",
    "corpus = root + \"ja-cn.gold\"\n",
    "output = root + \"ja-cn.simalign\"\n",
    "argmax_align_output = open(root + \"align.argmax\", \"w\")\n",
    "itermax_align_output = open(root + \"align.itermax\", \"w\")\n",
    "match_align_output = open(root + \"align.match\", \"w\")\n",
    "\n",
    "device =\"cpu\"\n",
    "model = \"bert-base-multilingual-cased\"\n",
    "model_path = \"/Users/lisan/code/bert-base-multilingual-cased\"\n",
    "distortion = 0.0\n",
    "null_align = 1.0\n",
    "\n",
    "\n",
    "model = SentenceAligner(model=model, model_path=model_path, device=device)\n",
    "\n",
    "\n",
    "src_sents = [line.split(\"\\t\")[0].strip() for line in open(corpus)]\n",
    "tgt_sents = [line.split(\"\\t\")[1].strip() for line in open(corpus)]\n",
    "\n",
    "index = 0\n",
    "for src, tgt in zip(src_sents, tgt_sents):\n",
    "    if index % 1 == 0: print(index)\n",
    "    result = model.get_word_aligns(src, tgt)\n",
    "    argmax_align_output.write(src + \"\\t\" + tgt + \"\\t\" + result[\"inter\"] + \"\\n\")\n",
    "    index += 1\n",
    "    \n",
    "        \n",
    "argmax_align_output.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "80208d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prec: 0.713\tRec: 0.748\tF1: 0.73\tAER: 0.27\n",
      "Prec: 0.896\tRec: 0.71\tF1: 0.792\tAER: 0.208\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_gold(g_path):\n",
    "    gold_f = open(g_path, \"r\").readlines()\n",
    "    pros = {}\n",
    "    surs = {}\n",
    "    all_count = 0.\n",
    "    surs_count = 0.\n",
    "    \n",
    "    for i, line in enumerate(gold_f):\n",
    "        line = line.strip().split(\"\\t\")[-1].split()\n",
    "\n",
    "        pros[i] = set([x.replace(\"p\", \"-\") for x in line]) # 以 p 或者 - 分开\n",
    "        surs[i] = set([x for x in line if \"p\" not in x])\n",
    "\n",
    "        all_count += len(pros[i])\n",
    "        surs_count += len(surs[i])\n",
    "\n",
    "    return pros, surs, surs_count\n",
    "\n",
    "def calc_score(input_path, probs, surs, surs_count):\n",
    "    total_hit = 0.\n",
    "    p_hit = 0.\n",
    "    s_hit = 0.\n",
    "    target_f = open(input_path, \"r\").readlines()\n",
    "\n",
    "    for i, line in enumerate(target_f):\n",
    "        line = line.strip().split(\"\\t\")[-1].split()\n",
    "\n",
    "        if i not in probs: continue\n",
    "        \n",
    "        if len(line[0].split(\"-\")) > 2:\n",
    "            line = [\"-\".join(x.split(\"-\")[:2]) for x in line]\n",
    "\n",
    "        p_hit += len(set(line) & set(probs[i]))\n",
    "        s_hit += len(set(line) & set(surs[i]))\n",
    "        total_hit += len(set(line))\n",
    "\n",
    "\n",
    "    y_prec = round(p_hit / max(total_hit, 1.), 3)\n",
    "    y_rec = round(s_hit / max(surs_count, 1.), 3)\n",
    "    y_f1 = round(2. * y_prec * y_rec / max((y_prec + y_rec), 0.01), 3)\n",
    "    aer = round(1 - (s_hit + p_hit) / (total_hit + surs_count), 3)\n",
    "\n",
    "    return y_prec, y_rec, y_f1, aer\n",
    "\n",
    "\n",
    "gold_path = \"/Users/lisan/code/AlignMan/examples/ja-cn.gold\"\n",
    "fastalign_path = \"/Users/lisan/code/AlignMan/examples/ja-cn.fastalign\"\n",
    "probs, surs, surs_count = load_gold(gold_path)\n",
    "y_prec, y_rec, y_f1, aer = calc_score(fastalign_path, probs, surs, surs_count)\n",
    "\n",
    "print(\"Prec: {}\\tRec: {}\\tF1: {}\\tAER: {}\".format(y_prec, y_rec, y_f1, aer))\n",
    "\n",
    "fastalign_path = \"/Users/lisan/code/AlignMan/examples/align.argmax\"\n",
    "probs, surs, surs_count = load_gold(gold_path)\n",
    "y_prec, y_rec, y_f1, aer = calc_score(fastalign_path, probs, surs, surs_count)\n",
    "\n",
    "print(\"Prec: {}\\tRec: {}\\tF1: {}\\tAER: {}\".format(y_prec, y_rec, y_f1, aer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c39e82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea4fb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import List, Text, Tuple\n",
    "\n",
    "\n",
    "def line2matrix(line: Text, n: int, m: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    '''\n",
    "    converts alignemnt given in the format \"0-1 3p4 5-6\" to alignment matrices\n",
    "    n, m: maximum length of the involved sentences (i.e., dimensions of the alignemnt matrices)\n",
    "    '''\n",
    "    def convert(i, j):\n",
    "        i, j = int(i), int(j)\n",
    "        if i >= n or j >= m:\n",
    "            raise ValueError(\"Error in Gold Standard?\")\n",
    "        return i, j\n",
    "    possibles = np.zeros((n, m))\n",
    "    sures = np.zeros((n, m))\n",
    "    for elem in line.split(\" \"):\n",
    "        if \"p\" in elem:\n",
    "            i, j = convert(*elem.split(\"p\"))\n",
    "            possibles[i, j] = 1\n",
    "        elif \"-\" in elem:\n",
    "            i, j = convert(*elem.split(\"-\"))\n",
    "            possibles[i, j] = 1\n",
    "            sures[i, j] = 1\n",
    "    return sures, possibles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
